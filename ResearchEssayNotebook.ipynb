{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('pydsEnv': conda)"
  },
  "interpreter": {
   "hash": "cd92519e450c25804136c2008eef24c3d70be2e96d599be5f96d22ea88f45fe2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Carlos Garcia - 21000475"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### K-Folds Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**K-fold.** Es una técnica de validación cruzada, cuyo objetivo es reducir el \"bias\" que puede presentarse al momento de entrenar un modelo de machine learning. Esta técnica se basa en el principio de evaluar el modelo sobre data que no ha sido utilizada para su entrenamiento, particionando el set de datos de entrenamiento para que este pueda ser usado para entrenamientos y validaciones. Su característica principal se basa en poder generar **K** particiones de la data, en las cuales cada una de estas particiones es utilizada para evaluar el modelo.\n",
    "De esta forma se entrenan sobre las K-1 y se evalua sobre la partición restante, repitiendo este proceso hasta recorrer todas las particiones generadas, los resultados obtenidos son promediados generando un estimador con menos sesgo para las métricas de evaluación a considerar.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src='./imgs/kfolds.png'>\n",
    "\n",
    "https://towardsdatascience.com/cross-validation-k-fold-vs-monte-carlo-e54df2fc179\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "En el proyecto de predicción de sobrevivientes del Titanic se trabajó con un split que generó sets de entrenamiento y pruebas. K-folds fue implementado al evitar particionar el dataset de entrenamiento en un set de validación adicional, es así como a partir del set de entrenamiento se trabajó con la función KFold de Scikit-learn la cual permitió particionar el dataset de entrenamiento K veces, de forma que se generaron predicciones sobre cada una de estas particiones y se promediaron los resultados. Siendo estos promedios los resultados guardados en la bitácora del proyecto, que posteriormente fueron utilizados para seleccionar los mejores modelos de cada tipo. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Es un algoritmo de Machine Learning que permite resolver problemas de clasificación lineal y no lineal, basado en la aplicación de vectores de soporte que determinan no solo una frontera de decisión si no también un margen de seperación para la tarea a realizar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./imgs/svm_graph.png\">\n",
    "\n",
    "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Hipótesis\n",
    "\n",
    "La hipótesis usada en SVM es una función por partes o condicional definida a partir de la la expresión lineal. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./imgs/svm_hypothesis.png\">\n",
    "\n",
    "*Hands-On Machine Learning with Scikit-Learn & Tensorflow*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Función de Costo\n",
    "\n",
    "La función de costos de SVM se conoce como bisagra(hinge).\n",
    "Esta función busca encontrar el punto mínimo de la curva en ciertos valores de otra curva. Es decir busca el punto mínimo a partir de una restricción que se relaciona con la función. (Optimización con restricciones)\n",
    "\n",
    "\n",
    "La funcion hinge no es un promedio de los errores si no más bien una suma de estos. En esta funcion el parámetro de regularización es denotado con la letra *C* y presenta un efecto igual al recíproco del término de regularización (1/lambda). \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./imgs/svm_costfunction.png\">\n",
    "\n",
    "*Hands-On Machine Learning with Scikit-Learn & Tensorflow*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Propiedades y comparación con otros algoritmos\n",
    "\n",
    "SVM es un algoritmo que, a diferencia de otros algoritmos (e.g. regresión logística), no define una frontera de decisión. Esto debido a que SVM es un algoritmo que computa a su vez vectores de soporte que buscan ampliar la brecha que separa una clase a la otra, permitiendo la configuración de este margen generando clasificasiones de margen suave y duro. \n",
    "\n",
    "**Ventajas**\n",
    "- Este algoritmo puede ser utilizado tanto para problemas de clasificasción como para problemas de regresión\n",
    "- Maneja problemas lineales y no lineales \n",
    "- La posibilidad de implementar kernel trick reduce significativamente la complejidad del modelo\n",
    "\n",
    "**Desventajas**\n",
    "- Si la data no esta escalada el modelo se ve altamente afectado\n",
    "- La aplicación de distintos kernel requiere del conocimiento de los distintos hiperparámetros (e.g. hiperparámetro C y Gamma) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Kernel trick\n",
    "\n",
    "Kernel es una funcion que mide la similitud entre dos vectores o funciones. Su aplicación permite generar un espacio latente, el cual es un espacio diferente obtenido a partir de ciertas transformaciones de los datos. \n",
    "\n",
    "El truco del kernel (kernel trick) es un concepto basado en *basis expansion* que permite replicar los efectos de transformar varias características sin tener que agregarlas realmente, evitando así el impacto computacional que genera el agregar una cierta cantidad de términos polinomiales. Esto con el objetivo de transformar las características en cuestión y generar un dataset separable para propósitos de la clasificación. \n",
    "\n",
    "En esencia el Kernel Trick se basa en ahorrar las transformacion de las características, de forma que el kernel es capaz de computar el producto punto de vectores transformados basado unicamente en los vectores originales, obviando así las transformaciones que serían necesarias de aplicar (phi). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Kernel trick**\n",
    "\n",
    "<img src=\"./imgs/kernel_trick.png\">\n",
    "\n",
    "*https://medium.com/@sonalij663/what-is-kernel-tricks-bb176291e60f*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Kernels**\n",
    "\n",
    "<img src=\"./imgs/svm_kernel.png\">\n",
    "\n",
    "*Hands-On Machine Learning with Scikit-Learn & Tensorflow*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Algoritmo de aprendizaje modelo SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\pydsEnv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from auxiliaryFunctions import getMetrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enabled compatitility to tf1.x\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  print(\"Enabled compatitility to tf1.x\")"
   ]
  },
  {
   "source": [
    "#### Datos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        SibSp     Parch      Fare  passenger_class  isFemale  \\\n",
       "329 -0.460103  0.829588  0.490420         1.599696         1   \n",
       "749 -0.460103 -0.474312 -0.472168        -0.815964         0   \n",
       "203 -0.460103 -0.474312 -0.482229        -0.815964         0   \n",
       "421 -0.460103 -0.474312 -0.472488        -0.815964         0   \n",
       "97  -0.460103  0.829588  0.593505         1.599696         0   \n",
       "\n",
       "     passenger_survived  \n",
       "329                   1  \n",
       "749                   0  \n",
       "203                   0  \n",
       "421                   0  \n",
       "97                    1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>passenger_class</th>\n      <th>isFemale</th>\n      <th>passenger_survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>329</th>\n      <td>-0.460103</td>\n      <td>0.829588</td>\n      <td>0.490420</td>\n      <td>1.599696</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>-0.460103</td>\n      <td>-0.474312</td>\n      <td>-0.472168</td>\n      <td>-0.815964</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>-0.460103</td>\n      <td>-0.474312</td>\n      <td>-0.482229</td>\n      <td>-0.815964</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>421</th>\n      <td>-0.460103</td>\n      <td>-0.474312</td>\n      <td>-0.472488</td>\n      <td>-0.815964</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>-0.460103</td>\n      <td>0.829588</td>\n      <td>0.593505</td>\n      <td>1.599696</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = pd.read_csv('./input/trainset_svm_essay.csv', index_col = 0)\n",
    "data.head()"
   ]
  },
  {
   "source": [
    "#### Entrenamiento"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x, y, epochs = 100, batch_size = 1, lr = 0.01, c = 0, kprint = 10):\n",
    "    \n",
    "    #Get total iterations\n",
    "    iters = x.shape[0] // batch_size\n",
    "\n",
    "    #Initialize the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    #Defining tensors and variables\n",
    "    X = tf.placeholder(dtype = tf.float32, shape = [None, x.shape[1] + 1], name = 'features')\n",
    "    labels = tf.placeholder(dtype = tf.float32, shape = [None, 1], name = 'class_labels')\n",
    "    W = tf.get_variable(name = 'weights', shape = [x.shape[1] + 1, 1], dtype = tf.float32, initializer = tf.zeros_initializer())\n",
    "\n",
    "    #Estimating values\n",
    "    logits = tf.matmul(X, W, name = 'logit')\n",
    "\n",
    "    #Calculating cost function (hinge)\n",
    "    with tf.name_scope('cost_function'):\n",
    "        margin_term = tf.multiply(tf.constant(1/2), tf.reduce_sum(tf.pow(W, 2)), name = 'margin_term')\n",
    "        hinge = tf.losses.hinge_loss(labels = labels, logits = logits)\n",
    "        penalizing_term = tf.multiply(c, hinge, name = 'penalizing_term')\n",
    "        loss = tf.add(margin_term, penalizing_term, name = 'loss')\n",
    "\n",
    "    #Tensorboard scalar summary\n",
    "    loss_summary = tf.summary.scalar(name = 'Hinge_Loss', tensor = loss)\n",
    "\n",
    "    #Calculate accuracy\n",
    "    with tf.name_scope('accuracy_definition'):\n",
    "        preds = tf.sign(logits)\n",
    "        t = tf.subtract(tf.multiply(labels, 2), tf.constant(1.0), name = 't_vector')\n",
    "        accuracy_tensor = tf.reduce_mean(tf.cast(tf.equal(preds, t),tf.float32), name = 'accuracy_metric')\n",
    "\n",
    "    #Tensorboard scalar accuracy\n",
    "    accuracy_summary = tf.summary.scalar(name = 'Accuracy', tensor = accuracy_tensor)\n",
    "\n",
    "    #Gradient and cost error optimization\n",
    "    with tf.name_scope('optimize_loss'):\n",
    "        gradients = tf.gradients(loss, [W], name = 'gradients')\n",
    "        optimizer = tf.assign(W, W - lr * gradients[0], name = 'optimizer')\n",
    "\n",
    "\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        #Initialize global vars\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        #Reshaping data\n",
    "        ones = np.expand_dims(np.ones_like(x[:,0]), axis = 1)\n",
    "        x = np.hstack((ones, x))\n",
    "        y = np.expand_dims(y, axis = 1)\n",
    "\n",
    "        #Whole batch dictionary\n",
    "        feed_dict_model = {X:x, labels:y}\n",
    "\n",
    "        #Define tensorboard writer\n",
    "        dt_string = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        writer = tf.summary.FileWriter('./graphs/svm/{}_svm_epochs={}_mbatch={}_lr={}_c={}_minmax'.format(dt_string, epochs, batch_size, lr, c), session.graph)\n",
    "\n",
    "        \n",
    "        for epoch in range(0, epochs):\n",
    "            for i in range(0, iters):\n",
    "                start_sample = i * batch_size\n",
    "                end_sample = start_sample + batch_size\n",
    "                x_mb = x[start_sample:end_sample]\n",
    "                y_mb = y[start_sample:end_sample]\n",
    "\n",
    "                feed_dict = {X:x_mb, labels:y_mb}\n",
    "                _, weights = session.run([optimizer, W], feed_dict = feed_dict)\n",
    "\n",
    "            predictions = session.run([preds], feed_dict = feed_dict_model)\n",
    "            cost, lsummary = session.run([loss, loss_summary], feed_dict = feed_dict_model)\n",
    "            writer.add_summary(lsummary, epoch + 1)\n",
    "\n",
    "            tvalue, accuracy, asummary = session.run([t, accuracy_tensor, accuracy_summary], feed_dict = feed_dict_model)\n",
    "            writer.add_summary(asummary, epoch + 1)\n",
    "\n",
    "            if (epoch + 1) % kprint == 0:\n",
    "                print(\"Epoch {}: HingeLoss={} --- Accuracy={}\".format(epoch + 1, cost, accuracy))\n",
    "        \n",
    "\n",
    "        #Calculate final model metrics\n",
    "        _, weights = session.run([optimizer, W], feed_dict = feed_dict_model)\n",
    "        predictions, cost = session.run([preds, loss], feed_dict = feed_dict_model)\n",
    "        tvalue, accuracy = session.run([t, accuracy_tensor], feed_dict = feed_dict_model)\n",
    "        print(\"Final model: HingeLoss={} --- Accuracy={}\".format(cost, accuracy))\n",
    "\n",
    "        #Close tensorboard writer\n",
    "        writer.close()\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 50: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 100: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 150: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 200: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 250: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 300: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 350: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 400: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 450: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 500: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 550: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 600: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 650: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 700: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 750: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 800: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 850: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 900: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 950: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Epoch 1000: HingeLoss=0.9643999338150024 --- Accuracy=0.699438214302063\n",
      "Final model: HingeLoss=0.9642010927200317 --- Accuracy=0.699438214302063\n"
     ]
    }
   ],
   "source": [
    "weights = trainModel(data.iloc[:,:-1].values, data.iloc[:,-1].values, epochs = 1000, batch_size = 200, lr = 0.1, c = 1.1, kprint = 50)"
   ]
  },
  {
   "source": [
    "#### Grafo SVM\n",
    "\n",
    "<img src=\"./imgs/svm_grafo.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Métricas en Tensorboard"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./imgs/hinge_loss.png\">\n",
    "\n",
    "<img src=\"./imgs/svm_accuracy.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Predicciones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        SibSp     Parch      Fare  passenger_class  isFemale  \\\n",
       "172  0.414308  0.582097 -0.534701        -0.874341         1   \n",
       "524 -0.536476 -0.481121 -0.637342        -0.874341         0   \n",
       "452 -0.536476 -0.481121 -0.097841         1.444281         0   \n",
       "170 -0.536476 -0.481121  0.053329         1.444281         0   \n",
       "620  0.414308 -0.481121 -0.447393        -0.874341         0   \n",
       "\n",
       "     passenger_survived  \n",
       "172                   1  \n",
       "524                   0  \n",
       "452                   0  \n",
       "170                   0  \n",
       "620                   0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>passenger_class</th>\n      <th>isFemale</th>\n      <th>passenger_survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>172</th>\n      <td>0.414308</td>\n      <td>0.582097</td>\n      <td>-0.534701</td>\n      <td>-0.874341</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>524</th>\n      <td>-0.536476</td>\n      <td>-0.481121</td>\n      <td>-0.637342</td>\n      <td>-0.874341</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>-0.536476</td>\n      <td>-0.481121</td>\n      <td>-0.097841</td>\n      <td>1.444281</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>-0.536476</td>\n      <td>-0.481121</td>\n      <td>0.053329</td>\n      <td>1.444281</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>0.414308</td>\n      <td>-0.481121</td>\n      <td>-0.447393</td>\n      <td>-0.874341</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "testset = pd.read_csv('./input/testset_svm_essay.csv', index_col = 0)\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(x, weights):\n",
    "    ones = np.expand_dims(np.ones_like(x[:,0]), axis = 1)\n",
    "    x = np.hstack((ones, x))\n",
    "    preds = np.sign(np.matmul(x, weights))\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = testset.iloc[:, -1].values\n",
    "y_labels = np.piecewise( y_labels, [y_labels == 0, y_labels > 0], [-1, 1])\n",
    "y_labels = np.expand_dims(y_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = getPredictions(testset.iloc[:, :-1].values, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            model  accuracy   error  precision  recall  f1-score\n",
       "0  tensorflow_SVM    0.7207  0.2793     0.6364  0.5385    0.5833"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>accuracy</th>\n      <th>error</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1-score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tensorflow_SVM</td>\n      <td>0.7207</td>\n      <td>0.2793</td>\n      <td>0.6364</td>\n      <td>0.5385</td>\n      <td>0.5833</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "metrics = getMetrics('tensorflow_SVM', y_labels, preds)\n",
    "metrics"
   ]
  }
 ]
}